{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this time using a Convolutional Neural Network that's better suited for image processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 1, 28, 28)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 1, 28, 28)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "    \n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "# to transform that number between 0 and 1\n",
    "train_images /= 255\n",
    "test_images /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to convert our train and test labels to be categorical in one-hot format:\n",
    "train_labels = tensorflow.keras.utils.to_categorical(mnist_train_labels, 10)\n",
    "test_labels = tensorflow.keras.utils.to_categorical(mnist_test_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print out one of the training images with its label:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_sample(num):\n",
    "    #Print the one-hot array of this sample's label \n",
    "    print(train_labels[num])  \n",
    "    #Print the label converted back to a number\n",
    "    label = train_labels[num].argmax(axis=0)\n",
    "    #Reshape the 768 values to a 28x28 image\n",
    "    image = train_images[num].reshape([28,28])\n",
    "    plt.title('Sample: %d  Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "    \n",
    "display_sample(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), # our convolution new layer going to have 32 windows\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "# 64 3x3 kernels\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# Reduce by taking the max of each 2x2 block\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Dropout to avoid overfitting\n",
    "model.add(Dropout(0.25))\n",
    "# Flatten the results to one dimension for passing into our final layer\n",
    "model.add(Flatten())\n",
    "# A hidden layer to learn with\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Another dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Final categorization from 0-9 with softmax\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 1101s - loss: 0.1888 - acc: 0.9432 - val_loss: 0.0455 - val_acc: 0.9848\n",
      "Epoch 2/10\n",
      " - 1108s - loss: 0.0766 - acc: 0.9766 - val_loss: 0.0348 - val_acc: 0.9888\n",
      "Epoch 3/10\n",
      " - 1073s - loss: 0.0598 - acc: 0.9818 - val_loss: 0.0319 - val_acc: 0.9890\n",
      "Epoch 4/10\n",
      " - 1061s - loss: 0.0492 - acc: 0.9850 - val_loss: 0.0287 - val_acc: 0.9903\n",
      "Epoch 5/10\n",
      " - 1054s - loss: 0.0423 - acc: 0.9867 - val_loss: 0.0287 - val_acc: 0.9907\n",
      "Epoch 6/10\n",
      " - 1043s - loss: 0.0342 - acc: 0.9894 - val_loss: 0.0263 - val_acc: 0.9916\n",
      "Epoch 7/10\n",
      " - 1041s - loss: 0.0320 - acc: 0.9898 - val_loss: 0.0280 - val_acc: 0.9921\n",
      "Epoch 8/10\n",
      " - 10985s - loss: 0.0288 - acc: 0.9908 - val_loss: 0.0288 - val_acc: 0.9916\n",
      "Epoch 9/10\n",
      " - 1068s - loss: 0.0263 - acc: 0.9916 - val_loss: 0.0242 - val_acc: 0.9931\n",
      "Epoch 10/10\n",
      " - 1098s - loss: 0.0246 - acc: 0.9919 - val_loss: 0.0294 - val_acc: 0.9922\n"
     ]
    }
   ],
   "source": [
    "#don't run, takes hours times to see result\n",
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you're building something where life and death are on the line, like a self-driving car, every fraction of a percent matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.029371827532436418\n",
      "Test accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
